
Alpha-beta剪枝:
Alpha-beta剪枝是一种搜索算法，用以减少极小化极大算法（Minimax算法）搜索树的节点数。这是一种对抗性搜索算法，主要应用于机器游玩的二人游戏（如井字棋、象棋、围棋）。当算法评估出某策略的后续走法比之前策略的还差时，就会停止计算该策略的后续发展。该算法和极小化极大算法所得结论相同，但剪去了不影响最终决定的分枝。

ADP(强化学习)
强调如何基于环境而行动，以取得最大化的预期利益。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。在机器学习问题中，环境通常被抽象为马尔可夫决策过程。
基本的强化学习被建模为马尔可夫决策过程：

环境状态的集合 S;
动作的集合 A;
在状态之间转换的规则（转移概率矩阵) P；
规定转换后“即时奖励”的规则（奖励函数) R；

UCT算法:
UCT算法（Upper Confidence Bound Apply to Tree），即上限置信区间算法，是一种博弈树搜索算法，该算法将蒙特卡洛树搜索(Monte—Carlo Tree Search，MCTS)方法与UCB公式结合，在超大规模博弈树的搜索过程中相对于传统的搜索算法有着时间和空间方面的优势。

MCTS(蒙特卡洛树搜索):
蒙特卡洛树搜索的每个循环包括四个步骤：

选择（Selection）：从根节点R开始，连续向下选择子节点至叶子节点L。下文将给出一种选择子节点的方法，让游戏树向最优的方向扩展，这是蒙特卡洛树搜索的精要所在。
扩展（Expansion）：除非任意一方的输赢使得游戏在L结束，否则创建一个或多个子节点并选取其中一个节点C。
仿真（Simulation）：在从节点C开始，用随机策略进行游戏，又称为playout或者rollout。
反向传播（Backpropagation）：使用随机游戏的结果，更新从C到R的路径上的节点信息。
每一个节点的内容代表胜利次数/游戏次数

来源:
https://zh.wikipedia.org/wiki/Alpha-beta%E5%89%AA%E6%9E%9D
https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0
https://zh.wikipedia.org/wiki/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2